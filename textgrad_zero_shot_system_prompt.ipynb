{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ä½¿ç”¨ Textgrad æœ€ä½³åŒ– system prompt ğŸ†\n",
        "\n",
        "\n",
        "\n",
        "é€™å€‹ Notebook ä½¿ç”¨ https://textgrad.com/ é€²è¡Œ system prompt çš„æœ€ä½³åŒ–\n",
        "\n",
        "* è¼¸å…¥ä½ çš„ä»»å‹™æè¿°\n",
        "* å°±èƒ½ç”¢å‡ºå¾ˆå²å®³ zero-shot prompt\n",
        "* ç”¨åœ¨æ²’æœ‰æ¨™æº–ç­”æ¡ˆçš„å ´æ™¯ï¼Œæ¡ç”¨ LLM è‡ªå‹•è©•ä¼°\n",
        "\n",
        "ä½œè€…å’Œæ¼”è¬›æŠ•å½±ç‰‡: ihower https://ihower.tw/blog/archives/12444\n",
        "\n",
        "### æµç¨‹\n",
        "\n",
        "1. ä½¿ç”¨ o1-preview åˆæˆè¨“ç·´å•é¡Œ\n",
        "2. ä½¿ç”¨ gpt-4o é€²è¡Œ textgrad æœ€ä½³åŒ–ï¼Œæ¡ç”¨ LLM-as-a-judge è‡ªå‹•åŒ–è©•ä¼°\n",
        "3. ç”¢ç”Ÿé©åˆ gpt-4o-mini çš„ system prompt\n",
        "\n",
        "æˆæœ¬: æœ€ä½³åŒ–è¿­ä»£å¤§ç´„è¦èŠ±5åˆ†é˜ï¼Œè€—è²» USD 0.8 ç¾é‡‘ (10å€‹è¨“ç·´ç¯„ä¾‹)"
      ],
      "metadata": {
        "id": "7Jo0Dea0J6o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. è¨­å®š OpenAI API key"
      ],
      "metadata": {
        "id": "W9B_btTuLCi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "è«‹é» google colab å·¦é‚Šå´æ¬„çš„é‘°åŒ™ç¬¦è™Ÿï¼Œæ–°å¢å¯†é‘°ï¼Œåç¨±æ˜¯ openai_api_keyï¼Œå€¼å°±å¡« API key"
      ],
      "metadata": {
        "id": "PV3zId96LEsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')"
      ],
      "metadata": {
        "id": "7balGFjoLBdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. è¨­å®šåƒæ•¸"
      ],
      "metadata": {
        "id": "odBHM6mtKNRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_model = \"o1-preview\" # åˆæˆè¨“ç·´å•é¡Œçš„æ¨¡å‹ï¼Œè‹¥ä½ æ²’æœ‰ o1 æ¬Šé™ï¼Œè«‹æ”¹ç”¨ gpt-4o\"\n",
        "generation_model = \"gpt-4o\" # åˆæˆ prompt çš„æ¨¡å‹\n",
        "prediction_model = \"gpt-4o-mini\" # ç”¨ä¾†åŸ·è¡Œ prompt çš„æ¨¡å‹\n",
        "\n",
        "task_description = \"æ ¹æ“šç”¨æˆ¶è¼¸å…¥çš„å°ˆæ¥­é ˜åŸŸï¼Œæ¢åˆ—å…¶ä¸­çš„é—œéµçŸ¥è­˜é‡é»\" # ä»»å‹™æè¿°ï¼Œè«‹ä¿®æ”¹æˆä½ çš„ä»»å‹™\n",
        "\n",
        "questions_num = 10  # è¦åˆæˆå¤šå°‘è¨“ç·´è³‡æ–™ï¼Œè·ŸèŠ±è²»çš„ API æˆæœ¬æœ‰é—œï¼Œå»ºè­°ä¸è¦å†å°‘äº†ï¼Œæœƒ overfitting\n",
        "\n",
        "# ç”¨ä¾†è©•ä¼°ç­”æ¡ˆå¥½ä¸å¥½çš„ promptï¼Œå¯ä»¥æ”¹ï¼Œä½†è«‹ä¿ç•™ [question_string] å­—ä¸²\n",
        "eval_prompt_template = \"\"\"Here's a question: [question_string].\n",
        "Evaluate any given answer to this question, be smart, logical, and very critical.\n",
        "Just provide concise feedback.\"\"\""
      ],
      "metadata": {
        "id": "Kn3CV3b_KDUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. åˆæˆæœ€ä½³åŒ–éœ€è¦çš„ dataset\n"
      ],
      "metadata": {
        "id": "BiEGAbY4IztH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_prompt = f\"\"\"You are tasked with creating a test dataset for an AI question-answering system. Your goal is to generate {questions_num} example questions based on a given task description. These questions should range from simple to complex, with the more difficult questions requiring reasoning and presenting a significant challenge.\n",
        "Here are the guidelines for generating the questions:\n",
        "\n",
        "Start with simple, straightforward questions and gradually increase the complexity.\n",
        "Ensure that the more difficult questions require multi-step reasoning or in-depth knowledge.\n",
        "Include a variety of question types (e.g., factual, analytical, hypothetical) relevant to the task description.\n",
        "Ensure that all questions are directly related to the provided task description.\n",
        "\n",
        "The task description you should base your questions on is as follows:\n",
        "<task_description>\n",
        "{task_description}\n",
        "</task_description>\n",
        "\n",
        "Please generate {questions_num} example questions based on this task description. Format your output as a JSON array of objects, where each object contains a 'question' key with the question text as its value, and an 'answer' key with the answer text as its value. The output should look like this:\n",
        "[\n",
        "{{\"question\": \"Question 1 text here\", \"answer\": \"Answer 1 text here\"}},\n",
        "{{\"question\": \"Question 2 text here\", \"answer\": \"Answer 2 text here\"}},\n",
        "...\n",
        "]\n",
        "\n",
        "Remember to increase the difficulty and complexity of the questions as you progress through the examples. The final few questions should be particularly challenging, requiring complex reasoning and demonstrating a high level of difficulty.\"\"\""
      ],
      "metadata": {
        "id": "JMjKvsE3CuJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install litellm\n",
        "\n",
        "from litellm import completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78T5L9i9D45D",
        "outputId": "ce704ea9-b50b-463b-a5d1-9e708529e58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting litellm\n",
            "  Downloading litellm-1.46.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm) (3.10.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (4.23.0)\n",
            "Collecting openai>=1.45.0 (from litellm)\n",
            "  Downloading openai-1.45.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (2.9.1)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (2.32.3)\n",
            "Collecting tiktoken>=0.7.0 (from litellm)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm) (0.19.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->litellm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.45.0->litellm) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.45.0->litellm)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.45.0->litellm)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->litellm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->litellm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->litellm) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2024.8.30)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.7.0->litellm) (2024.5.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (4.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm) (0.24.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.45.0->litellm) (1.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.45.0->litellm)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.45.0->litellm)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
            "Downloading litellm-1.46.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.45.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, jiter, h11, tiktoken, httpcore, httpx, openai, litellm\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 litellm-1.46.0 openai-1.45.0 python-dotenv-1.0.1 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    { \"content\": synthetic_prompt, \"role\": \"user\"}\n",
        "]\n",
        "\n",
        "if not synthetic_model.startswith('o1'):\n",
        "  response = completion(model=synthetic_model, messages=messages, response_format={ \"type\": \"json_object\" })\n",
        "else:\n",
        "  # o1 ç›®å‰é‚„ä¸æ”¯æ´ json mode\n",
        "  response = completion(model=synthetic_model, messages=messages)\n",
        "\n",
        "response = response.choices[0].message.content\n",
        "dataset = json.loads(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvULyE7CD8ED",
        "outputId": "b5e1bf80-501f-41ad-8fb6-1af0be19325a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:387: UserWarning: Pydantic serializer warnings:\n",
            "  Expected `CompletionTokensDetails` but got `dict` with value `{'reasoning_tokens': 4480}` - serialized value may not be as expected\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejQqTthWFsdR",
        "outputId": "48eff436-50c9-49b6-e98b-f8ee22a7dae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'question': '1. è¿™ä¸ªä»»åŠ¡çš„ä¸»è¦ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ', 'answer': 'æ ¹æ®ç”¨æˆ·è¾“å…¥çš„ä¸“ä¸šé¢†åŸŸï¼Œåˆ—å‡ºå…¶ä¸­çš„å…³é”®çŸ¥è¯†é‡ç‚¹ã€‚'},\n",
              " {'question': '2. å¦‚æœç”¨æˆ·è¾“å…¥â€œè®¡ç®—æœºç§‘å­¦â€ä½œä¸ºä¸“ä¸šé¢†åŸŸï¼Œåº”è¯¥åˆ—å‡ºå“ªäº›å…³é”®çŸ¥è¯†ç‚¹ï¼Ÿ',\n",
              "  'answer': 'è®¡ç®—æœºç§‘å­¦çš„å…³é”®çŸ¥è¯†ç‚¹åŒ…æ‹¬ç®—æ³•ã€æ•°æ®ç»“æ„ã€è®¡ç®—æœºç»„æˆåŸç†ã€æ“ä½œç³»ç»Ÿã€ç¼–ç¨‹è¯­è¨€ã€æ•°æ®åº“ã€ç½‘ç»œã€è½¯ä»¶å·¥ç¨‹åŸç†å’Œäººå·¥æ™ºèƒ½ç­‰ã€‚'},\n",
              " {'question': '3. å¦‚ä½•åˆ¤æ–­æŸä¸ªçŸ¥è¯†ç‚¹åœ¨ä¸€ä¸ªä¸“ä¸šé¢†åŸŸå†…æ˜¯å…³é”®çš„ï¼Ÿ',\n",
              "  'answer': 'å¯ä»¥æ ¹æ®å…¶åŸºç¡€æ€§ã€å¯¹å¤šä¸ªå­é¢†åŸŸçš„ç›¸å…³æ€§ã€åœ¨ä¸“ä¸šä¸­ä½¿ç”¨çš„é¢‘ç‡ã€ä½œä¸ºé«˜çº§ä¸»é¢˜çš„å‰ææ¡ä»¶çš„ä½œç”¨ï¼Œä»¥åŠå…¶åœ¨æ ‡å‡†è¯¾ç¨‹å’Œè®¤è¯ä¸­çš„åŒ…å«æƒ…å†µæ¥åˆ¤æ–­ã€‚'},\n",
              " {'question': '4. å¦‚ä½•ç¡®ä¿åˆ—å‡ºçš„å…³é”®çŸ¥è¯†ç‚¹æ¸…å•æ˜¯å…¨é¢ä¸”å‡†ç¡®çš„ï¼Ÿ',\n",
              "  'answer': 'å¯ä»¥å‚è€ƒæƒå¨æ¥æºï¼Œå¦‚å­¦æœ¯è¯¾ç¨‹ã€è¡Œä¸šæ ‡å‡†ã€ä¸“ä¸šæŒ‡å—ã€æ•™ç§‘ä¹¦ã€ä¸“å®¶æ„è§ï¼Œå¹¶é€šè¿‡äº¤å‰æ£€éªŒå¤šä¸ªå¯ä¿¡èµ„æºæ¥éªŒè¯æ¯ä¸ªçŸ¥è¯†ç‚¹çš„é‡è¦æ€§ã€‚'},\n",
              " {'question': '5. æè¿°ä¸€ç§å°†ä¸“ä¸šé¢†åŸŸçš„å…³é”®çŸ¥è¯†ç‚¹è¿›è¡Œç»“æ„åŒ–ç»„ç»‡çš„æ–¹æ³•ã€‚',\n",
              "  'answer': 'ä¸€ç§æ–¹æ³•æ˜¯å°†çŸ¥è¯†ç‚¹åˆ†ç±»åˆ°è¯¥ä¸“ä¸šçš„ä¸»è¦é¢†åŸŸæˆ–å­é¢†åŸŸï¼ŒæŒ‰ç…§ä»ä¸€èˆ¬åˆ°å…·ä½“çš„å±‚æ¬¡ç»“æ„æ’åˆ—ï¼Œæˆ–æ ¹æ®ä¸»é¢˜ã€æ¦‚å¿µæˆ–èƒ½åŠ›è¿›è¡Œåˆ†ç»„ï¼Œä»¥æä¾›é€»è¾‘æ€§çš„ç»“æ„ã€‚'},\n",
              " {'question': '6. ä¸ºä»€ä¹ˆæ ¹æ®ç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚æˆ–èƒŒæ™¯å®šåˆ¶å…³é”®çŸ¥è¯†ç‚¹åˆ—è¡¨å¾ˆé‡è¦ï¼Ÿ',\n",
              "  'answer': 'å› ä¸ºä¸åŒçš„ç”¨æˆ·å¯èƒ½å…·æœ‰ä¸åŒçš„ä¸“ä¸šæ°´å¹³ã€å…·ä½“å…´è¶£æˆ–ç‰¹å®šåº”ç”¨ï¼Œå®šåˆ¶åˆ—è¡¨å¯ä»¥æé«˜å…¶ç›¸å…³æ€§å’Œå®ç”¨æ€§ï¼Œå¢å¼ºå…¶æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚'},\n",
              " {'question': '7. å¦‚æœè¦ä¸ºä¸€ä¸ªèµ„æºæœ‰é™çš„ä¸“ä¸šå°ä¼—é¢†åŸŸåˆ—å‡ºå…³é”®çŸ¥è¯†ç‚¹ï¼Œä½ å°†å¦‚ä½•ç€æ‰‹ï¼Ÿ',\n",
              "  'answer': 'æˆ‘ä¼šè”ç³»è¯¥é¢†åŸŸçš„ä¸“å®¶ï¼Œå®¡é˜…ä»»ä½•å¯ç”¨çš„æ–‡çŒ®æˆ–æ¡ˆä¾‹ç ”ç©¶ï¼Œåˆ†æç›¸å…³çš„æ›´å¹¿æ³›çš„é¢†åŸŸä»¥å¯»æ‰¾é‡å çš„çŸ¥è¯†ç‚¹ï¼Œå¹¶åˆ©ç”¨ä¸“ä¸šäººå£«è®¨è®ºç›¸å…³ä¸»é¢˜çš„åœ¨çº¿ç¤¾åŒºæˆ–è®ºå›ã€‚'},\n",
              " {'question': '8. è®¨è®ºåœ¨è¯†åˆ«è·¨å­¦ç§‘ä¸“ä¸šé¢†åŸŸçš„å…³é”®çŸ¥è¯†ç‚¹æ—¶å¯èƒ½é‡åˆ°çš„æŒ‘æˆ˜ï¼Œä»¥åŠå¦‚ä½•å…‹æœå®ƒä»¬ã€‚',\n",
              "  'answer': 'æŒ‘æˆ˜åŒ…æ‹¬æ•´åˆå¤šä¸ªå­¦ç§‘çš„æ¦‚å¿µçš„å¤æ‚æ€§ã€æœ¯è¯­æˆ–æ–¹æ³•è®ºçš„æ½œåœ¨å†²çªï¼Œä»¥åŠæ‰€éœ€çŸ¥è¯†çš„å¹¿åº¦ã€‚å…‹æœè¿™äº›æŒ‘æˆ˜éœ€è¦æ·±å…¥ç ”ç©¶ï¼Œä¸å„å­¦ç§‘ä¸“å®¶åˆä½œï¼Œä»”ç»†ç»¼åˆä¿¡æ¯ï¼Œçªå‡ºæœ€å…³é”®çš„äº¤å‰ç‚¹ã€‚'},\n",
              " {'question': '9. å¦‚ä½•éªŒè¯æ‰€è¯†åˆ«çš„å…³é”®çŸ¥è¯†ç‚¹æ˜¯æœ€æ–°çš„ï¼Œåæ˜ äº†ä¸“ä¸šé¢†åŸŸçš„å½“å‰è¶‹åŠ¿å’Œè¿›å±•ï¼Ÿ',\n",
              "  'answer': 'å¯ä»¥é€šè¿‡å®¡é˜…æœ€æ–°å‡ºç‰ˆç‰©ã€å‚åŠ ä¸“ä¸šä¼šè®®ã€å…³æ³¨è¡Œä¸šæ–°é—»å’Œæ›´æ–°ã€å’¨è¯¢å½“ä»£ä¸“å®¶ï¼Œä»¥åŠæ£€æŸ¥è¯¥é¢†åŸŸæƒå¨æœºæ„å‘å¸ƒçš„æœ€æ–°æ ‡å‡†æˆ–æœ€ä½³å®è·µæ¥è¿›è¡ŒéªŒè¯ã€‚'},\n",
              " {'question': '10. æå‡ºä¸€ä¸ªç³»ç»Ÿçš„æ–¹æ³•æ¥å¼€å‘ä¸€ä¸ªç®—æ³•æˆ–å·¥å…·ï¼Œèƒ½å¤Ÿä¸ºä»»ä½•ç»™å®šçš„ä¸“ä¸šé¢†åŸŸç”Ÿæˆå…³é”®çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼Œè¯¦ç»†è¯´æ˜æ‰€æ¶‰åŠçš„æ­¥éª¤ä»¥åŠå¦‚ä½•åº”å¯¹å„ç§æŒ‘æˆ˜ã€‚',\n",
              "  'answer': 'ä¸€ä¸ªç³»ç»Ÿçš„æ–¹æ³•å¯èƒ½åŒ…æ‹¬ï¼š\\n\\n1. **è¾“å…¥åˆ†æ**ï¼šæ¥å—ä¸“ä¸šé¢†åŸŸä½œä¸ºè¾“å…¥ï¼Œæ ‡å‡†åŒ–æœ¯è¯­ï¼Œè¯†åˆ«ç›¸å…³çš„å…³é”®è¯ã€‚\\n\\n2. **æ•°æ®æ”¶é›†**ï¼šä»æƒå¨æ¥æºæ”¶é›†æ•°æ®ï¼Œå¦‚ä¸“ä¸šåä¼šã€å­¦æœ¯æœºæ„å’Œè¡Œä¸šå‡ºç‰ˆç‰©ã€‚\\n\\n3. **çŸ¥è¯†æå–**ï¼šä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ä»æ”¶é›†çš„æ•°æ®ä¸­æå–æ½œåœ¨çš„çŸ¥è¯†ç‚¹ã€‚\\n\\n4. **ç›¸å…³æ€§è¿‡æ»¤**ï¼šåº”ç”¨æ ‡å‡†è¯„ä¼°æ¯ä¸ªçŸ¥è¯†ç‚¹çš„é‡è¦æ€§ï¼Œå¦‚æåŠé¢‘ç‡ã€åœ¨é¢†åŸŸä¸­çš„æ ¸å¿ƒåœ°ä½å’Œä¸“å®¶éªŒè¯ã€‚\\n\\n5. **åˆ†ç±»æ•´ç†**ï¼šå°†çŸ¥è¯†ç‚¹ç»„ç»‡æˆé€»è¾‘ç±»åˆ«å’Œå±‚æ¬¡ç»“æ„ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£ã€‚\\n\\n6. **æŒç»­æ›´æ–°**ï¼šå®æ–½åé¦ˆæœºåˆ¶ï¼ŒæŒç»­ç”¨æ–°ä¿¡æ¯æ›´æ–°åˆ—è¡¨ï¼Œåˆ é™¤è¿‡æ—¶çš„çŸ¥è¯†ç‚¹ã€‚\\n\\nè¿™ç§æ–¹æ³•é€šè¿‡åˆ©ç”¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•å’Œä¸“å®¶è¾“å…¥ï¼Œç¡®ä¿ç”Ÿæˆçš„çŸ¥è¯†ç‚¹å‡†ç¡®ã€ç›¸å…³ä¸”åŠæ—¶ï¼Œä»è€Œåº”å¯¹å„ç§æŒ‘æˆ˜ã€‚'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ä½¿ç”¨ Textgrad æœ€ä½³åŒ– system prompt"
      ],
      "metadata": {
        "id": "q_IQhxwaFhDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textgrad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsPYE348Bjxm",
        "outputId": "f353b3dc-d1d1-45b8-f57f-2779db5a0204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textgrad\n",
            "  Downloading textgrad-0.1.5.tar.gz (65 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/65.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m61.4/65.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: openai>=1.23.6 in /usr/local/lib/python3.10/dist-packages (from textgrad) (1.45.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from textgrad) (9.0.0)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from textgrad) (1.0.1)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from textgrad) (2.1.4)\n",
            "Requirement already satisfied: platformdirs>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from textgrad) (4.3.2)\n",
            "Collecting datasets>=2.14.6 (from textgrad)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting diskcache>=5.6.3 (from textgrad)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: graphviz>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from textgrad) (0.20.3)\n",
            "Collecting gdown>=5.2.0 (from textgrad)\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from textgrad) (9.4.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from textgrad) (0.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->textgrad) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->textgrad) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.14.6->textgrad)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.6->textgrad)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->textgrad) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->textgrad) (4.66.5)\n",
            "Collecting xxhash (from datasets>=2.14.6->textgrad)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.14.6->textgrad)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.14.6->textgrad) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->textgrad) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->textgrad) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->textgrad) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->textgrad) (6.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=5.2.0->textgrad) (4.12.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.23.6->textgrad) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.23.6->textgrad) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.23.6->textgrad) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.23.6->textgrad) (2.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.23.6->textgrad) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.23.6->textgrad) (4.12.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->textgrad) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->textgrad) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->textgrad) (3.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->textgrad) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.3->textgrad) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.3->textgrad) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.3->textgrad) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.23.6->textgrad) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->textgrad) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->textgrad) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->textgrad) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->textgrad) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->textgrad) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->textgrad) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->textgrad) (4.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.23.6->textgrad) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.23.6->textgrad) (2.23.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.3->textgrad) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=5.2.0->textgrad) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=5.2.0->textgrad) (1.7.1)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: textgrad\n",
            "  Building wheel for textgrad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textgrad: filename=textgrad-0.1.5-py3-none-any.whl size=69628 sha256=6dd1513566c6c3c5f595f9c7307d65cf1ff343897551b8d35716147a261a6bc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/6a/60/e64ce939ea4985107c5a3d20113da49c2694951b12a7267d00\n",
            "Successfully built textgrad\n",
            "Installing collected packages: xxhash, pyarrow, diskcache, dill, multiprocess, gdown, datasets, textgrad\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.1.0\n",
            "    Uninstalling gdown-5.1.0:\n",
            "      Successfully uninstalled gdown-5.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 diskcache-5.6.3 gdown-5.2.0 multiprocess-0.70.16 pyarrow-17.0.0 textgrad-0.1.5 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textgrad as tg\n",
        "from textgrad.tasks import load_task\n",
        "\n",
        "llm_engine = tg.get_engine(prediction_model, override=True )\n",
        "tg.set_backward_engine(generation_model, override=True )\n",
        "\n",
        "system_prompt = tg.Variable(\"You are a concise LLM.\",\n",
        "                            requires_grad=True,\n",
        "                            role_description=\"system prompt to guide the LLM's reasoning strategy for accurate responses\")\n",
        "\n",
        "model = tg.BlackboxLLM(llm_engine, system_prompt=system_prompt)\n",
        "optimizer = tg.TGD(parameters=list(model.parameters()))"
      ],
      "metadata": {
        "id": "kWZz32xHBgp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# é–‹å§‹è·‘æœ€ä½³åŒ–è¿­ä»£\n",
        "for data in dataset:\n",
        "    question_string = data[\"question\"]\n",
        "    question = tg.Variable(question_string, role_description=\"question to the LLM\", requires_grad=False)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    prediction = model(question)\n",
        "    prediction.set_role_description(\"concise and accurate answer to the question\")\n",
        "\n",
        "    evaluation_instruction = eval_prompt_template.replace( '[question_string]', question_string)\n",
        "    loss_fn = tg.TextLoss(evaluation_instruction)\n",
        "    loss = loss_fn(prediction)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yncfcG2YGFvw",
        "outputId": "ab2874a1-2871-4d4b-dbcb-7eab6ea39966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# è¼¸å‡ºæœ€çµ‚çš„ system prompt çµæœ\n",
        "print(system_prompt.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxpEwg4OIDvF",
        "outputId": "30c14274-06c9-4c85-b5d4-d4d917a0706c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a concise LLM that provides clear, specific, direct, and accurate information to help users solve problems or complete tasks. Follow these guidelines:\n",
            "\n",
            "1. **Foundational and Industry-Relevant Knowledge**:\n",
            "   - Prioritize critical information first.\n",
            "   - Use consistent terminology and detail for each category.\n",
            "   - Reference authoritative bodies and industry standards (e.g., ISO, ASTM).\n",
            "   - Include high-impact journals, open-access journals, and preprint servers like arXiv and bioRxiv to capture the latest research.\n",
            "   - Ensure data comprehensiveness and representativeness by selecting diverse publication types and geographic diversity.\n",
            "\n",
            "2. **Domain Definition**:\n",
            "   - Provide criteria for determining the domain's boundaries, such as scope of research topics, geographical limitations, or specific industry applications.\n",
            "\n",
            "3. **Audience Understanding**:\n",
            "   - Suggest methods like preliminary surveys, interviews, or focus groups to gather detailed information about the audience's background and specific knowledge requirements.\n",
            "\n",
            "4. **Resource Collection Techniques**:\n",
            "   - Mention specific databases (e.g., PubMed, IEEE Xplore, JSTOR, Scopus, Web of Science) and techniques like citation tracking to identify key papers and authors.\n",
            "\n",
            "5. **Examples and Real-World Applications**:\n",
            "   - Provide specific examples for each point to enhance understanding and relatability.\n",
            "   - Mention interdisciplinary projects like the Human Genome Project.\n",
            "\n",
            "6. **Emerging Trends and Future Directions**:\n",
            "   - Discuss trends, future directions, and ongoing research.\n",
            "   - Highlight interdisciplinary connections and broader impacts.\n",
            "\n",
            "7. **Evaluation Metrics**:\n",
            "   - Mention citation metrics and industry adoption rates.\n",
            "   - Discuss the importance of peer-reviewed journals.\n",
            "   - Include metrics like h-index, citation counts, precision, recall, and F1-score to gauge research influence and performance.\n",
            "\n",
            "8. **Practical Implementation**:\n",
            "   - Provide step-by-step guides for complex processes.\n",
            "   - Include criteria for selecting experts and methods for outreach.\n",
            "\n",
            "9. **Feedback Mechanism**:\n",
            "   - Establish feedback channels (e.g., surveys, focus groups).\n",
            "   - Describe how to analyze and incorporate feedback systematically.\n",
            "\n",
            "10. **Tools and Platforms**:\n",
            "    - Mention specific tools (e.g., Mendeley for literature review, MindMeister for mind maps).\n",
            "    - Recommend tools like RSS feeds (e.g., Feedly), academic alert services (e.g., Mendeley Alerts), and platforms like Zotero or WordPress for managing references and creating resource hubs.\n",
            "    - Offer brief guides on using these tools effectively, including tips on creating mind maps and organizing information visually.\n",
            "\n",
            "11. **Case Studies and Examples**:\n",
            "    - Describe how projects like the Human Genome Project implemented steps.\n",
            "    - Suggest criteria for selecting case studies (e.g., significance, availability of detailed information) and provide a template for presenting them.\n",
            "    - Include diverse case studies from various fields to illustrate the broad applicability of the strategies.\n",
            "    - For each example, provide a detailed analysis, explaining the specific challenges faced and how they were overcome.\n",
            "\n",
            "12. **Future Trends**:\n",
            "    - Include emerging technologies and methodologies (e.g., AI tools, big data platforms).\n",
            "\n",
            "13. **Visual Aids**:\n",
            "    - Use diagrams, flowcharts, or tables to illustrate steps and processes.\n",
            "\n",
            "14. **Terminology**:\n",
            "    - Define technical terms clearly within the text and use consistent terminology throughout.\n",
            "    - Manage polysemy, synonyms, and domain-specific terminology using domain-specific lexicons, context-aware embeddings, and synonym dictionaries.\n",
            "\n",
            "15. **Brevity and Clarity**:\n",
            "    - Ensure each point is succinct and free of redundant phrases.\n",
            "    - Maintain conciseness by avoiding redundant information and focusing on the most critical points.\n",
            "\n",
            "16. **Logical Flow**:\n",
            "    - Arrange points in a logical sequence to enhance the flow of information.\n",
            "    - Ensure that the response follows a logical structure, starting with an introduction, followed by a detailed discussion, and concluding with a summary of key points.\n",
            "    - Use clear transitions between sections to guide the reader through the argument and maintain coherence.\n",
            "\n",
            "17. **Quantitative Evidence**:\n",
            "    - Where possible, include quantitative evidence or studies to support your points.\n",
            "    - Briefly address potential counterarguments or limitations to provide a balanced view.\n",
            "\n",
            "18. **Engaging Language**:\n",
            "    - Use engaging and dynamic language to keep the reader's interest.\n",
            "\n",
            "19. **Feedback Integration**:\n",
            "    - Incorporate user feedback to continuously improve the relevance and accuracy of your responses.\n",
            "\n",
            "20. **Sharing Platforms and Engagement Strategies**:\n",
            "    - Suggest specific platforms relevant to the niche field (e.g., LinkedIn groups, Reddit communities, ResearchGate, Academia.edu) and provide strategies for engaging with these communities.\n",
            "    - Encourage active participation in conferences, such as presenting papers or joining panel discussions, to gain deeper insights and networking opportunities.\n",
            "\n",
            "21. **Challenge Analysis**:\n",
            "    - For each challenge, provide a detailed explanation, including specific examples and implications for the domain.\n",
            "    - Describe how to collaborate with domain experts (e.g., regular workshops, expert panels) and detail the process of cross-validating multiple algorithms (e.g., ensemble methods, consensus scoring).\n",
            "\n",
            "22. **Strategy Implementation**:\n",
            "    - For each strategy, provide specific, actionable steps and tools that can be used to implement the strategy effectively.\n",
            "\n",
            "23. **Risk Identification and Mitigation**:\n",
            "    - Identify potential risks or limitations associated with each strategy and discuss how they can be mitigated.\n",
            "    - Propose methods to mitigate identified risks, such as fostering trust and collaboration among team members.\n",
            "\n",
            "24. **Technical Feasibility and Project Management**:\n",
            "    - Elaborate on the interdisciplinary team's structure, roles, and collaboration methods.\n",
            "    - Include project management practices such as agile methodologies, regular sprints, and clear milestones to ensure effective implementation.\n",
            "\n",
            "By following these guidelines, you will generate accurate and helpful responses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»¥ä¸‹æ˜¯é€™å€‹ system prompt ä¸­æ–‡ç¿»è­¯ä¾›å°ç…§: https://chatgpt.com/share/66ea86da-4040-8008-a2f9-cc5806fa5f05"
      ],
      "metadata": {
        "id": "Uow-7cLNHN0g"
      }
    }
  ]
}