# -*- coding: utf-8 -*-
"""textgrad-zero-shot-system-prompt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PB4Y5LXNz19vHlFhoBlz86VChVM0Nsqf

## ä½¿ç”¨ Textgrad æœ€ä½³åŒ– system prompt ğŸ†



é€™å€‹ Notebook ä½¿ç”¨ https://textgrad.com/ é€²è¡Œ system prompt çš„æœ€ä½³åŒ–

* è¼¸å…¥ä½ çš„ä»»å‹™æè¿°
* å°±èƒ½ç”¢å‡ºå¾ˆå²å®³ zero-shot prompt
* ç”¨åœ¨æ²’æœ‰æ¨™æº–ç­”æ¡ˆçš„å ´æ™¯ï¼Œæ¡ç”¨ LLM è‡ªå‹•è©•ä¼°

ä½œè€…å’Œæ¼”è¬›æŠ•å½±ç‰‡: ihower https://ihower.tw/blog/archives/12444

### æµç¨‹

1. ä½¿ç”¨ o1-preview åˆæˆè¨“ç·´å•é¡Œ
2. ä½¿ç”¨ gpt-4o é€²è¡Œ textgrad æœ€ä½³åŒ–ï¼Œæ¡ç”¨ LLM-as-a-judge è‡ªå‹•åŒ–è©•ä¼°
3. ç”¢ç”Ÿé©åˆ gpt-4o-mini çš„ system prompt

æˆæœ¬: æœ€ä½³åŒ–è¿­ä»£å¤§ç´„è¦èŠ±5åˆ†é˜ï¼Œè€—è²» USD 0.8 ç¾é‡‘ (10å€‹è¨“ç·´ç¯„ä¾‹)

## 0. è¨­å®š OpenAI API key

è«‹é» google colab å·¦é‚Šå´æ¬„çš„é‘°åŒ™ç¬¦è™Ÿï¼Œæ–°å¢å¯†é‘°ï¼Œåç¨±æ˜¯ openai_api_keyï¼Œå€¼å°±å¡« API key
"""

from google.colab import userdata
import os
import json

os.environ["OPENAI_API_KEY"] = userdata.get('openai_api_key')

"""## 1. è¨­å®šåƒæ•¸"""

synthetic_model = "o1-preview" # åˆæˆè¨“ç·´å•é¡Œçš„æ¨¡å‹ï¼Œè‹¥ä½ æ²’æœ‰ o1 æ¬Šé™ï¼Œè«‹æ”¹ç”¨ gpt-4o"
generation_model = "gpt-4o" # åˆæˆ prompt çš„æ¨¡å‹
prediction_model = "gpt-4o-mini" # ç”¨ä¾†åŸ·è¡Œ prompt çš„æ¨¡å‹

task_description = "æ ¹æ“šç”¨æˆ¶è¼¸å…¥çš„å°ˆæ¥­é ˜åŸŸï¼Œæ¢åˆ—å…¶ä¸­çš„é—œéµçŸ¥è­˜é‡é»" # ä»»å‹™æè¿°ï¼Œè«‹ä¿®æ”¹æˆä½ çš„ä»»å‹™

questions_num = 10  # è¦åˆæˆå¤šå°‘è¨“ç·´è³‡æ–™ï¼Œè·ŸèŠ±è²»çš„ API æˆæœ¬æœ‰é—œï¼Œå»ºè­°ä¸è¦å†å°‘äº†ï¼Œæœƒ overfitting

# ç”¨ä¾†è©•ä¼°ç­”æ¡ˆå¥½ä¸å¥½çš„ promptï¼Œå¯ä»¥æ”¹ï¼Œä½†è«‹ä¿ç•™ [question_string] å­—ä¸²
eval_prompt_template = """Here's a question: [question_string].
Evaluate any given answer to this question, be smart, logical, and very critical.
Just provide concise feedback."""

"""## 2. åˆæˆæœ€ä½³åŒ–éœ€è¦çš„ dataset

"""

synthetic_prompt = f"""You are tasked with creating a test dataset for an AI question-answering system. Your goal is to generate {questions_num} example questions based on a given task description. These questions should range from simple to complex, with the more difficult questions requiring reasoning and presenting a significant challenge.
Here are the guidelines for generating the questions:

Start with simple, straightforward questions and gradually increase the complexity.
Ensure that the more difficult questions require multi-step reasoning or in-depth knowledge.
Include a variety of question types (e.g., factual, analytical, hypothetical) relevant to the task description.
Ensure that all questions are directly related to the provided task description.

The task description you should base your questions on is as follows:
<task_description>
{task_description}
</task_description>

Please generate {questions_num} example questions based on this task description. Format your output as a JSON array of objects, where each object contains a 'question' key with the question text as its value, and an 'answer' key with the answer text as its value. The output should look like this:
[
{{"question": "Question 1 text here", "answer": "Answer 1 text here"}},
{{"question": "Question 2 text here", "answer": "Answer 2 text here"}},
...
]

Remember to increase the difficulty and complexity of the questions as you progress through the examples. The final few questions should be particularly challenging, requiring complex reasoning and demonstrating a high level of difficulty."""

!pip install litellm

from litellm import completion

messages = [
    { "content": synthetic_prompt, "role": "user"}
]

if not synthetic_model.startswith('o1'):
  response = completion(model=synthetic_model, messages=messages, response_format={ "type": "json_object" })
else:
  # o1 ç›®å‰é‚„ä¸æ”¯æ´ json mode
  response = completion(model=synthetic_model, messages=messages)

response = response.choices[0].message.content
dataset = json.loads(response)

dataset

"""## 3. ä½¿ç”¨ Textgrad æœ€ä½³åŒ– system prompt"""

!pip install textgrad

import textgrad as tg
from textgrad.tasks import load_task

llm_engine = tg.get_engine(prediction_model, override=True )
tg.set_backward_engine(generation_model, override=True )

system_prompt = tg.Variable("You are a concise LLM.",
                            requires_grad=True,
                            role_description="system prompt to guide the LLM's reasoning strategy for accurate responses")

model = tg.BlackboxLLM(llm_engine, system_prompt=system_prompt)
optimizer = tg.TGD(parameters=list(model.parameters()))

# é–‹å§‹è·‘æœ€ä½³åŒ–è¿­ä»£
for data in dataset:
    question_string = data["question"]
    question = tg.Variable(question_string, role_description="question to the LLM", requires_grad=False)

    optimizer.zero_grad()
    prediction = model(question)
    prediction.set_role_description("concise and accurate answer to the question")

    evaluation_instruction = eval_prompt_template.replace( '[question_string]', question_string)
    loss_fn = tg.TextLoss(evaluation_instruction)
    loss = loss_fn(prediction)

    loss.backward()
    optimizer.step()

# è¼¸å‡ºæœ€çµ‚çš„ system prompt çµæœ
print(system_prompt.value)

"""ä»¥ä¸‹æ˜¯é€™å€‹ system prompt ä¸­æ–‡ç¿»è­¯ä¾›å°ç…§: https://chatgpt.com/share/66ea86da-4040-8008-a2f9-cc5806fa5f05"""